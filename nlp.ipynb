{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp",
      "provenance": [],
      "authorship_tag": "ABX9TyOETYSA8V/wht5hc6JXTZKq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikramnist/Hate-Speech-Classification-deployed-using-Flask/blob/master/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faFcovQ5VlM3",
        "outputId": "ded2b3e1-4e92-49db-bae1-73c0f6e0dbd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "import csv\n",
        "import os\n",
        "import requests, sys\n",
        "import pandas as pd\n",
        "from sumy.parsers.html import HtmlParser\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer as Lsa\n",
        "from sumy.summarizers.luhn import LuhnSummarizer as Luhn\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer as TxtRank\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer as LexRank\n",
        "from sumy.summarizers.sum_basic import SumBasicSummarizer as SumBasic\n",
        "from sumy.summarizers.kl import KLSummarizer as KL\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from sumy.utils import get_stop_words\n",
        "\n",
        "SENTENCES_COUNT = 1\n",
        "\n",
        "# We call the script with 3 arguments: \n",
        "# 1. the CSV with the list of URLs to analyze, \n",
        "# 2. the new CSV where we will store the new MDs and \n",
        "# 3. the language to be used (if missing \"english\" will be used)\n",
        "# i.e. \"generate-md.py in.csv out.csv english\" \n",
        "\n",
        "urlinput = sys.argv[1]\n",
        "print(\"csv to analyze: \", urlinput)\n",
        "outputcsv = sys.argv[2]\n",
        "print(\"output csv name: \", outputcsv)\n",
        "\n",
        "# Check if language has been set\n",
        "def get_lan():\n",
        "    try:\n",
        "        sys.argv[3]\n",
        "    except IndexError:\n",
        "        return 'english'\n",
        "    else:\n",
        "        return sys.argv[3]\n",
        "\n",
        "LANGUAGE = get_lan()\n",
        "\n",
        "print(\"language set to: \", LANGUAGE)\n",
        "\n",
        "# Open the CSV file with the list of URLs to analyze\n",
        "df = pd.read_csv('student_new.csv')\n",
        "\n",
        "print(\"Number of rows in csv\", len(df)) \n",
        "\n",
        "# Create a list to store the MDs\n",
        "data_x = [] \n",
        "\n",
        "# For each URL in the input CSV run the analysis and store the results in the list \n",
        "for i in range(len(df)):\n",
        "    stemmer = Stemmer(LANGUAGE)\n",
        "    # Here is the URL to be analyzed\n",
        "    line = df.iloc[i][0]\n",
        "    print(line)\n",
        "\n",
        "    lsaSummarizer = Lsa(stemmer)\n",
        "    lsaSummarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "    \n",
        "\t# Error handling for HTTP connection problems\n",
        "    try:\n",
        "       parser = HtmlParser.from_url(line, Tokenizer(LANGUAGE))\n",
        "    except:\n",
        "    \tprint('error while fetching', line)\n",
        "\n",
        "\t# LSA\n",
        "    for lsa_sentence in lsaSummarizer(parser.document, SENTENCES_COUNT):\n",
        "    \tprint(lsa_sentence)\n",
        "    print(\"Summarizing URL via LSA: \" + line)\n",
        "\t\n",
        "\n",
        "\n",
        "\n",
        "\t# Storing all values into the list \n",
        "    data_x.append({\"url\":line, \"DESC\":lsa_sentence})\n",
        "    \n",
        "\n",
        "# Save results to the output CSV\n",
        "df = pd.DataFrame(data_x, columns=[\"url\", \"DESC\"])\n",
        "df.to_csv('outputc.csv', encoding='utf-8', index=False)\n",
        "print(\"Results saved on\", outputcsv)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "csv to analyze:  -f\n",
            "output csv name:  /root/.local/share/jupyter/runtime/kernel-316276e7-8ef5-475a-80a2-49250d25fcce.json\n",
            "language set to:  english\n",
            "Number of rows in csv 2\n",
            "https://timesofindia.indiatimes.com/city/delhi/brace-for-water-supply-at-low-pressure-for-next-few-days/articleshow/78737891.cms\n",
            "error while fetching https://timesofindia.indiatimes.com/city/delhi/brace-for-water-supply-at-low-pressure-for-next-few-days/articleshow/78737891.cms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3450bad2e87b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# LSA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mlsa_sentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlsaSummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSENTENCES_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsa_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Summarizing URL via LSA: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'parser' is not defined"
          ]
        }
      ]
    }
  ]
}